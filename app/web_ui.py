import streamlit as st
import os
import sys
import html

# Add project root to path for Streamlit Cloud
# Use insert(0) to ensure this path takes precedence
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import json
import time
import io
import itertools
from collections import Counter

from core.db import get_all_data, init_db
# These were unused in the UI and causing ImportErrors due to missing/moved functions
# from core.analysis import analyze_sentiment, detailed_aspect_analysis

from config.settings import GAMES
import jieba
import re
import sqlite3

# --- Segmentation Optimization ---
@st.cache_resource
def init_segmentation():
    """Add hero names and sentiment keywords to jieba for better segmentation."""
    # 1. From GAMES keywords
    for game in GAMES.values():
        if 'keywords' in game:
            for word in game['keywords'].keys():
                if len(word) > 1:
                    jieba.add_word(word)
    
    # 2. Strong sentiment keywords for better identification
    strong_keywords = ["è´µå¾—è¦æ­»", "åƒç›¸éš¾çœ‹", "å‰²éŸ­èœ", "ç™½å«–", "é€ç¦è¢‹", "è¿˜åŸåº¦", "åŒ¹é…æœºåˆ¶", "è¿è´¥", "è¿èƒœ"]
    for kw in strong_keywords:
        jieba.add_word(kw)
    
    return True

init_segmentation()

# --- Advanced Tokenization Setup ---
try:
    import nltk
    from nltk.stem import WordNetLemmatizer
    from pythainlp import word_tokenize as thai_tokenize
    # Download necessary NLTK data
    nltk.download('wordnet', quiet=True)
    nltk.download('omw-1.4', quiet=True)
    lemmatizer = WordNetLemmatizer()
    HAS_SPECIALIZED = True
except ImportError:
    HAS_SPECIALIZED = False

def smart_tokenize(text, source=None, stopwords=None):
    """
    Advanced multi-language tokenization:
    - Thai: pythainlp (Dictionary-based)
    - English: NLTK Lemmatization + Regex
    - Chinese: Jieba
    """
    if not text or not isinstance(text, str):
        return []
    
    text = text.lower()
    
    # 1. Detect Thai Content (Thai characters: \u0E00-\u0E7F)
    if re.search(r'[\u0E00-\u0E7F]', text):
        if HAS_SPECIALIZED:
            words = thai_tokenize(text, engine="newmm")
        else:
            words = re.findall(r'[\u0E00-\u0E7F]+', text)
            
    # 2. Detect Chinese Content (Simplified & Traditional)
    # Using a broader range \u4e00-\u9fff and source hints
    elif re.search(r'[\u4e00-\u9fff]', text) or (source and source.lower() in ['youtube', 'bahamut', 'discord', 'baidutieba']):
        words = list(jieba.cut(text))
        
    # 3. Primarily International / English
    else:
        # Regex for words
        raw_words = re.findall(r'\b[a-z]{2,}\b', text)
        if HAS_SPECIALIZED:
            # Lemmatize English words
            words = [lemmatizer.lemmatize(w) for w in raw_words]
        else:
            words = raw_words

    # 4. Global Filtering
    if stopwords:
        # Keep single characters if they are Chinese (very important for sentiment like 'è´µ', 'å¡', 'å‘')
        return [w.strip() for w in words if (len(w.strip()) > 1 or re.search(r'[\u4e00-\u9fa5]', w)) and w.strip() not in stopwords and not re.match(r'^[0-9.]+$', w)]
    return [w.strip() for w in words if (len(w.strip()) > 1 or re.search(r'[\u4e00-\u9fa5]', w)) and not re.match(r'^[0-9.]+$', w)]


def load_stopwords():
    stop_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config', 'stopwords.txt')
    if os.path.exists(stop_path):
        with open(stop_path, 'r', encoding='utf-8') as f:
            return set([line.strip() for line in f if line.strip()])
    return set()

def format_tooltip(meta):
    if not meta:
        return ""
    full_c = meta.get('full_content', '')
    # Remove extra spaces/newlines to keep it compact
    clean_c = " ".join(full_c.split())
    # Truncate to 100 characters as requested
    if len(clean_c) > 100:
        clean_c = clean_c[:100] + "..."
    
    # Safe quotes for HTML attributes
    clean_c = clean_c.replace("'", 'â€™').replace('"', 'â€')
    source = meta.get('source', 'æœªçŸ¥')
    date = meta.get('date', 'æœªçŸ¥')
    
    return html.escape(f"å®Œæ•´è¯„è®º: {clean_c}\næ¥æº: {source}\næ—¶é—´: {date}")

def load_events():
    events_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config', 'events.json')
    if os.path.exists(events_path):
        try:
            with open(events_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except: pass
    return []

def add_events_to_fig(fig, events, start_date, end_date):
    if not events: return fig
    # Normalize filter dates
    s_dt = pd.to_datetime(start_date)
    e_dt = pd.to_datetime(end_date)
    
    for ev in events:
        try:
            ev_start = pd.to_datetime(ev['start'])
            ev_end = pd.to_datetime(ev['end'])
            
            # Show if event overlaps with filter
            if (ev_start <= e_dt) and (ev_end >= s_dt):
                fig.add_vrect(
                    x0=max(ev_start, s_dt), 
                    x1=min(ev_end, e_dt),
                    fillcolor=ev.get('color', 'rgba(150, 150, 150, 0.15)'),
                    layer="below", line_width=0,
                    annotation_text=ev['name'],
                    annotation_position="top left",
                    annotation=dict(font_size=10, font_color="#666", bgcolor="white", opacity=0.8)
                )
        except: continue
    return fig

# Page Configuration
st.set_page_config(
    page_title="Multi-Game Monitor",
    page_icon="ğŸ®",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Authentication
def check_password():
    """Returns `True` if the user is authorized."""
    
    # 1. Check for public/local mode (no secrets)
    if "DB_USERNAME" not in st.secrets:
        return True

    # 2. Check if already authenticated via Session State
    if st.session_state.get("password_correct", False):
        return True
    
    # 3. Show Login Form
    st.header("Login")
    with st.form("auth_form"):
        # Use new keys to avoid conflicts with old session state
        input_user = st.text_input("Username", key="auth_username")
        input_pass = st.text_input("Password", type="password", key="auth_password")
        submitted = st.form_submit_button("Log in")

    if submitted:
        # Check credentials
        is_valid = False
        
        # 1. Check legacy (DB_USERNAME)
        if (
            "DB_USERNAME" in st.secrets 
            and input_user == st.secrets["DB_USERNAME"]
            and input_pass == st.secrets["DB_TOKEN"]
        ):
            is_valid = True
            
        # 2. Check multiple users ([passwords] section)
        elif "passwords" in st.secrets:
            # secrets["passwords"] returns a AttrDict/Dict
            if input_user in st.secrets["passwords"] and input_pass == st.secrets["passwords"][input_user]:
                is_valid = True
                
        if is_valid:
            st.session_state["password_correct"] = True
            st.rerun()
        else:
            st.error("ğŸ˜• User not known or password incorrect")
            
    return False

if not check_password():
    st.info("Please log in to continue.")
    st.stop()

# --- Helper Functions (New Style) ---
def render_hero(title, subtitle="Sentiment Analysis System"):
    st.markdown(f"""
        <div class="hero-box">
            <h1 style="margin: 0; font-size: 2.4em; letter-spacing: 0.05em; font-family: 'Noto Serif JP', serif;">{title}</h1>
            <div style="height: 2px; background: #9F353A; width: 60px; margin: 20px 0;"></div>
            <p style="color: #666; font-size: 1em; text-transform: uppercase; letter-spacing: 0.15em; font-weight: 500;">{subtitle}</p>
        </div>
    """, unsafe_allow_html=True)

# Traditional Japanese Color Palette
JP_COLORS = ["#165E83", "#9F353A", "#D0AF4C", "#1B4D3E", "#70649A", "#B14B28"]

# Custom CSS
# Custom CSS
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Noto+Serif+JP:wght@700&family=Noto+Sans+SC:wght@400;500;700&family=Inter:wght@400;600&display=swap');
    
    :root {
        --header-font: 'Noto Serif JP', 'Noto Sans SC', serif;
        --body-font: 'Inter', 'Noto Sans SC', sans-serif;
        --accent-color: #9F353A;
    }

    .main {
        background-color: #F9F7F2;
        color: #2D2D2D;
        font-family: var(--body-font);
    }
    
    /* Standardized Header Styles */
    h1 {
        font-family: var(--header-font) !important;
        font-size: 2.2rem !important;
        color: #1A1A1A !important;
        font-weight: 700 !important;
        margin-bottom: 0.5rem !important;
        letter-spacing: -0.02em !important;
    }
    
    h2 {
        font-family: var(--header-font) !important;
        font-size: 1.6rem !important;
        color: #333 !important;
        border-bottom: 2px solid #E0DED7;
        padding-bottom: 8px;
        margin-top: 2rem !important;
        margin-bottom: 1.2rem !important;
    }
    
    h3 {
        font-family: var(--header-font) !important;
        font-size: 1.25rem !important;
        color: var(--accent-color) !important;
        margin-top: 1.5rem !important;
        margin-bottom: 0.8rem !important;
    }

    .stMetric label {
        font-family: var(--body-font) !important;
        font-weight: 600 !important;
        text-transform: uppercase;
        letter-spacing: 0.05em;
        color: #666;
    }
    
    .stMetric [data-testid="stMetricValue"] {
        font-family: var(--header-font) !important;
        font-size: 1.8rem !important;
    }

    .hero-box {
        background: #FFFFFF;
        padding: 30px;
        border-left: 6px solid var(--accent-color);
        margin-bottom: 40px;
        border-radius: 4px;
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05);
    }

    /* Chart improvement - no border */
    .plot-container {
        border: none;
        background: transparent;
        padding: 0;
    }

    /* Feedback Box Styles */
    .feedback-box {
        padding: 12px 16px;
        border-radius: 8px;
        margin-bottom: 10px;
        font-size: 0.92rem;
        line-height: 1.6;
        border: 1px solid #EAE6DF;
        transition: all 0.2s ease;
        background: #FFFFFF;
        box-shadow: 0 2px 4px rgba(0,0,0,0.03);
    }
    .feedback-box:hover {
        box-shadow: 0 4px 12px rgba(0,0,0,0.08);
        transform: translateY(-1px);
    }
    .feedback-pos {
        border-left: 5px solid #2ecc71;
    }
    .feedback-neg {
        border-left: 5px solid #e74c3c;
    }
    .mode-tag {
        display: inline-block;
        padding: 2px 8px;
        background: #f0f0f0;
        border-radius: 4px;
        font-size: 0.75rem;
        color: #555;
        margin-right: 8px;
        font-weight: 600;
        vertical-align: middle;
        border: 1px solid #ddd;
    }
</style>
""", unsafe_allow_html=True)

@st.cache_data(ttl=60)
def load_data(game_filter=None):
    init_db()
    df = get_all_data()
    if not df.empty:
        if 'review_date' in df.columns:
            df['review_date'] = pd.to_datetime(df['review_date'], errors='coerce')
        if 'sentiment_score' in df.columns:
            df['sentiment_score'] = pd.to_numeric(df['sentiment_score'], errors='coerce')
        
        # Filter by game
        if game_filter:
            # Handle legacy data with no game_id as jump_assemble
            if 'game_id' not in df.columns:
                 df['game_id'] = 'jump_assemble'
            else:
                 df['game_id'] = df['game_id'].fillna('jump_assemble')
                 
            df = df[df['game_id'] == game_filter]
            
    return df



# Sidebar
with st.sidebar:
    st.title("èˆ†æƒ…ç›‘æ§ä¸­å¿ƒ")
    
    # Game Selector
    game_keys = list(GAMES.keys())
    game_names = [GAMES[k]['name'] for k in game_keys]
    selected_game_name = st.selectbox("é€‰æ‹©é¡¹ç›®", game_names)
    selected_game_key = game_keys[game_names.index(selected_game_name)]
    
    st.caption(f"å½“å‰é¡¹ç›®: {selected_game_name}")
    st.markdown("---")
    
    # Navigation (Top Priority)
    menu = st.radio("å¯¼èˆª", ["ğŸ“Š æ€»è§ˆå¤§å±", "ğŸ§­ è¯­ä¹‰é€è§†ä¸æœç´¢", "ğŸ“š æ¼«ç”» IP å¤§ç›˜", "ğŸ¦¸ è‹±é›„ä¸“é¡¹", "âš™ï¸ ç©æ³•åé¦ˆ", "ğŸ“„ åˆ†ææœˆæŠ¥", "ğŸ”§ é…ç½®ç®¡ç†"], index=0)
    st.markdown("---")
    
    # Load Data for Sidebar Filters
    df = load_data(selected_game_key)
    
    # Date Filter
    st.subheader("ğŸ“… æ—¶é—´ç­›é€‰")
    today = pd.Timestamp.now().date()
    # Default to last 1 year
    start_date = st.date_input("å¼€å§‹æ—¥æœŸ", today - pd.Timedelta(days=365))
    end_date = st.date_input("ç»“æŸæ—¥æœŸ", today)
    
    # Source Filter
    st.subheader("ğŸŒ æ¥æºç­›é€‰")
    all_sources = sorted(list(df['source'].dropna().unique())) if 'source' in df.columns else []
    selected_sources = st.multiselect("é€‰æ‹©æ¥æº", all_sources, default=all_sources)
    
    if "taptap_intl" in selected_sources:
        pass

    st.markdown("---")
    # Removed Download CSV/XLSX section as requested due to performance lag

# Filter by Source
if not df.empty and 'source' in df.columns and selected_sources:
    df = df[df['source'].isin(selected_sources)]

# Apply Date Filter
if not df.empty and 'review_date' in df.columns:
    # Convert inputs to datetime
    s_dt = pd.to_datetime(start_date)
    e_dt = pd.to_datetime(end_date) + pd.Timedelta(days=1) - pd.Timedelta(seconds=1) # End of day
    
    mask = (df['review_date'] >= s_dt) & (df['review_date'] <= e_dt)
    df = df.loc[mask]


@st.cache_data(ttl=300)
def load_hero_ip_map(game_key):
    """Load mapping of Hero Code -> IP Group Name and Display Name based on heroes.json"""
    config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config', 'heroes.json')
    hero_ip_map = {} # HeroCode -> IPName
    ip_hero_list = {} # IPName -> [HeroCodes]
    hero_display_map = {} # HeroCode -> First Alias (Display Name)
    
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                fc = json.load(f)
                if game_key in fc and "Groups" in fc[game_key]:
                    groups = fc[game_key]["Groups"]
                    for g_name, g_content in groups.items():
                        # g_content is {HeroCode: [Aliases]}
                        codes = list(g_content.keys())
                        ip_hero_list[g_name] = codes
                        for c, aliases in g_content.items():
                            hero_ip_map[c] = g_name
                            # The first alias is usually the primary Chinese name
                            hero_display_map[c] = aliases[0] if aliases else c
                            
        except Exception as e:
            print(f"Error loading hero map: {e}")
            
    return hero_ip_map, ip_hero_list, hero_display_map

@st.cache_data(ttl=300)
def process_trends(df, hero_ip_map):
    """Process raw reviews into IP and Hero trend data."""
    if df.empty or 'detailed_analysis' not in df.columns:
        return pd.DataFrame(), pd.DataFrame()
        
    ip_records = []
    hero_records = []
    
    for _, row in df.iterrows():
        try:
            if pd.isna(row['detailed_analysis']): continue
            data = json.loads(row['detailed_analysis'])
            heroes = data.get("Heroes", {})
            
            review_date = row['review_date']
            sentiment = row['sentiment_score'] if 'sentiment_score' in row else 0.5
            
            # Track IPs mentioned in this review to avoid double counting IP heat per review (optional, but let's count all mentions)
            # Actually, if a review mentions Goku and Vegeta, it counts twice for Dragon Ball? Usually yes for "Heat".
            
            for h_code in heroes.keys():
                ip_group = hero_ip_map.get(h_code, "Unknown")
                
                # Hero Record
                hero_records.append({
                    "date": review_date,
                    "hero": h_code,
                    "ip": ip_group,
                    "sentiment": sentiment,
                    "count": 1
                })
                
                # IP Record
                if ip_group != "Unknown":
                    ip_records.append({
                        "date": review_date,
                        "ip": ip_group,
                        "sentiment": sentiment,
                        "count": 1
                    })
                    
        except:
            continue
            
    return pd.DataFrame(ip_records), pd.DataFrame(hero_records)

if menu == "ğŸ“Š æ€»è§ˆå¤§å±":
    render_hero(f"{selected_game_name} Overview", "èˆ†æƒ…æ€»è§ˆ")
    if df.empty:
        st.warning("æš‚æ— æ•°æ®ï¼Œè¯·å»çˆ¬è™«æ§åˆ¶å°æŠ“å–ã€‚")
    else:
        # Top Metrics
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("æ€»è¯„è®ºæ•°", len(df))
        c2.metric("å¹³å‡æƒ…æ„Ÿ", f"{df['sentiment_score'].mean():.2f}" if 'sentiment_score' in df.columns else "0")
        
        # Sentiment Chart
        c1, c2 = st.columns(2)
        with c1:
            st.subheader("ğŸ˜Š æƒ…æ„Ÿå€¾å‘")
            if 'sentiment_label' in df.columns:
                counts = df['sentiment_label'].value_counts().reset_index()
                counts.columns = ['Label', 'Count']
                fig = px.pie(counts, values='Count', names='Label', color='Label',
                             title='ç©å®¶æƒ…æ„Ÿæ„æˆåˆ†å¸ƒ',
                             color_discrete_map={'Positive':'#2ecc71', 'Negative':'#e74c3c', 'Neutral':'#95a5a6'})
                fig.update_layout(
                    height=350,
                    font_family="Inter", 
                    title_font=dict(size=18, family="Noto Serif JP", color="#1A1A1A"),
                    margin=dict(t=60, b=20, l=40, r=40),
                    paper_bgcolor='rgba(0,0,0,0)',
                    plot_bgcolor='rgba(0,0,0,0)'
                )
                st.plotly_chart(fig, use_container_width=True)
        
        with c2:
            st.subheader("â­ è¯„åˆ†åˆ†å¸ƒ")
            if 'rating' in df.columns:
                rc = df['rating'].value_counts().sort_index().reset_index()
                rc.columns = ['Star', 'Count']
                rc['Star'] = rc['Star'].replace({0: 'æœŸå¾…/0æ˜Ÿ'})
                fig_star = px.bar(rc, x='Star', y='Count', 
                                 title='App Store / TapTap è¯„åˆ†åˆ†å¸ƒ',
                                 template="plotly_white")
                fig_star.update_traces(marker_color=JP_COLORS[0], marker_line_color='#bcaf9f', marker_line_width=1)
                fig_star.update_layout(
                    height=350,
                    font_family="Inter", 
                    title_font=dict(size=18, family="Noto Serif JP", color="#1A1A1A"),
                    margin=dict(t=60, b=20, l=40, r=40),
                    xaxis=dict(title=None, showgrid=False),
                    yaxis=dict(title=None, gridcolor="#EEE"),
                    paper_bgcolor='rgba(0,0,0,0)',
                    plot_bgcolor='rgba(0,0,0,0)'
                )
                st.plotly_chart(fig_star, use_container_width=True)

        # 1.25 è¶‹åŠ¿å…³è”åˆ†æ (Trend Analysis)
        st.markdown("---")
        st.subheader("ğŸ“‰ èˆ†æƒ…æ³¢åŠ¨ä¸äº‹ä»¶å…³è”")
        
        events = load_events()
        if not df.empty:
            # Prepare Daily Stats
            daily_stats = df.groupby(df['review_date'].dt.date).agg({
                'id': 'count',
                'sentiment_score': 'mean'
            }).reset_index()
            daily_stats.columns = ['date', 'count', 'sentiment']
            
            # Sub-header for Event Info
            if events:
                evt_list = []
                for ev in events:
                    if (pd.to_datetime(ev['start']) <= pd.to_datetime(end_date)) and (pd.to_datetime(ev['end']) >= pd.to_datetime(start_date)):
                        evt_list.append(f"`{ev['name']}` ({ev['start']} ~ {ev['end']})")
                if evt_list:
                    st.caption(f"å½“å‰æ—¶æ®µå…³è”äº‹ä»¶: {' | '.join(evt_list)}")

            # Draw Dual Axis Chart
            fig_trend = go.Figure()
            # Volume (Bar)
            fig_trend.add_trace(go.Bar(
                x=daily_stats['date'], y=daily_stats['count'], name='è¯„è®ºé‡',
                marker_color='rgba(200, 200, 200, 0.3)', yaxis='y'
            ))
            # Sentiment (Line)
            fig_trend.add_trace(go.Scatter(
                x=daily_stats['date'], y=daily_stats['sentiment'], name='å¹³å‡æƒ…æ„Ÿ',
                line=dict(color=JP_COLORS[1], width=3), yaxis='y2'
            ))
            
            # Add Events
            fig_trend = add_events_to_fig(fig_trend, events, start_date, end_date)
            
            fig_trend.update_layout(
                title="èˆ†æƒ…è¶‹åŠ¿ä¸å…³é”®äº‹ä»¶å…³è”åˆ†æ",
                height=350,
                template='plotly_white',
                font_family="Inter", title_font_family="Noto Serif JP",
                hovermode='x unified',
                margin=dict(t=30, b=20),
                legend=dict(orientation="v", yanchor="top", y=1, xanchor="left", x=1.02),
                yaxis=dict(title='è¯„è®ºæ•°', side='left', showgrid=False),
                yaxis2=dict(title='æƒ…æ„Ÿåˆ†', side='right', overlaying='y', range=[0, 1], showgrid=True, gridcolor="#f0f0f0"),
                xaxis=dict(title=None, showgrid=False)
            )
            st.plotly_chart(fig_trend, use_container_width=True)

        # 1. Activity Heatmap (Standard GitHub Contribution Graph)
        st.markdown("---")
        st.subheader("ğŸ—“ï¸ è¯„è®ºæ´»è·ƒåº¦")
        
        if not df.empty and 'review_date' in df.columns:
            import numpy as np
            
            # 1. Data Preparation
            h_df = df.copy()
            h_df['review_date'] = pd.to_datetime(h_df['review_date']).dt.normalize()
            
            # 2. Generate Full Range
            all_dates = pd.date_range(start=start_date, end=end_date)
            base_df = pd.DataFrame({'review_date': all_dates})
            base_df['day_of_week'] = base_df['review_date'].dt.dayofweek # 0=Mon, 6=Sun
            base_df['month_name'] = base_df['review_date'].dt.strftime('%b')
            base_df['date_str'] = base_df['review_date'].dt.strftime('%b %d, %Y')
            
            # 3. Align to Weeks (Monday-based columns)
            # Find the starting Monday for the entire grid
            start_of_grid = all_dates.min() - pd.Timedelta(days=all_dates.min().dayofweek)
            base_df['week_num'] = (base_df['review_date'] - start_of_grid).dt.days // 7
            
            # 4. Aggregate Actual Data
            agg = h_df.groupby('review_date').size().reset_index(name='count')
            merged = pd.merge(base_df, agg, on='review_date', how='left').fillna(0)
            
            # 5. Pivot for Layout
            # Rows: Weekday (0-6), Columns: Week Index
            pivot_z = merged.pivot(index='day_of_week', columns='week_num', values='count').fillna(0)
            pivot_date = merged.pivot(index='day_of_week', columns='week_num', values='date_str')
            
            unique_weeks = sorted(merged['week_num'].unique())
            
            # 6. Prepare X-axis Month Labels
            x_labels = []
            last_m = ""
            for wn in unique_weeks:
                # Get the month of the first day of this week
                m = merged[merged['week_num'] == wn]['month_name'].iloc[0]
                if m != last_m:
                    x_labels.append(m)
                    last_m = m
                else:
                    x_labels.append("")

            # 7. Custom Hover Text
            custom_hover = []
            for r in range(7):
                row_hover = []
                for c in range(len(unique_weeks)):
                    d_str = pivot_date.iloc[r, c] if c < pivot_date.shape[1] else ""
                    cnt = int(pivot_z.iloc[r, c]) if c < pivot_z.shape[1] else 0
                    if d_str:
                        row_hover.append(f"{d_str}<br>{cnt} reviews")
                    else:
                        row_hover.append("")
                custom_hover.append(row_hover)

            # 8. Render using go.Heatmap for precise control
            colorscale = ["#ebedf0", "#9be9a8", "#40c463", "#30a14e", "#216e39"] # GitHub Greens
            
            fig = go.Figure(data=go.Heatmap(
                z=pivot_z.values,
                x=list(range(len(unique_weeks))),
                y=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'],
                colorscale=colorscale,
                showscale=False,
                xgap=3,
                ygap=3,
                hoverinfo="text",
                text=custom_hover
            ))
            
            fig.update_layout(
                height=220,
                margin=dict(t=40, b=20, l=40, r=20),
                plot_bgcolor='rgba(0,0,0,0)',
                xaxis=dict(
                    tickmode='array',
                    tickvals=list(range(len(unique_weeks))),
                    ticktext=x_labels,
                    side='top',
                    showgrid=False,
                    zeroline=False,
                    fixedrange=True
                ),
                yaxis=dict(
                    autorange="reversed",
                    tickmode='array',
                    tickvals=[1, 3, 5],
                    ticktext=["Mon", "Wed", "Fri"],
                    showgrid=False,
                    zeroline=False,
                    fixedrange=True
                )
            )
            # Ensure square cells
            fig.update_yaxes(scaleanchor="x", scaleratio=1)
            
            st.plotly_chart(fig, use_container_width=True, config={'displayModeBar': False})

        # 1.5 çƒ­ç‚¹æ¼”è¿›è·¯çº¿ (Topic Evolution)
        st.markdown("---")
        st.subheader("ğŸ“ˆ æ ¸å¿ƒè¯é¢˜æ¼”è¿›è¶‹åŠ¿")
        
        if not df.empty and 'detailed_analysis' in df.columns:
            # å‡†å¤‡æ•°æ®ï¼šæå–æ—¥æœŸå’Œç³»ç»Ÿç»´åº¦
            topic_trend_data = []
            for _, row in df.iterrows():
                if pd.isna(row['detailed_analysis']): continue
                try:
                    analysis = json.loads(row['detailed_analysis'])
                    # Aggregate by day
                    date = pd.to_datetime(row['review_date']).normalize()
                    
                    # ç»Ÿè®¡ç³»ç»Ÿç»´åº¦ (Optimization, Network, Matchmaking, Welfare)
                    system_aspects = analysis.get("System", {})
                    for aspect, items in system_aspects.items():
                        if items: # è¯¥è¯„è®ºæåˆ°äº†è¿™ä¸ªç»´åº¦
                            topic_trend_data.append({"date": date, "topic": aspect, "count": 1})
                except:
                    continue
            
            if topic_trend_data:
                trend_df = pd.DataFrame(topic_trend_data)
                
                # Apply date filter
                s_dt_norm = pd.to_datetime(start_date).normalize()
                e_dt_norm = pd.to_datetime(end_date).normalize()
                trend_df = trend_df[(trend_df['date'] >= s_dt_norm) & (trend_df['date'] <= e_dt_norm)]
                
                if not trend_df.empty:
                    # Time Aggregation Selector
                    agg_col, _ = st.columns([1, 4])
                    with agg_col:
                        agg_type = st.radio("æ—¶é—´èšåˆ", ["æ—¥", "å‘¨", "æœˆ"], index=1, horizontal=True)
                    
                    # Frequency Map
                    freq_map = {"æ—¥": "D", "å‘¨": "W-MON", "æœˆ": "M"} # Use 'M' for months in to_period
                    
                    # Aggregation Logic
                    bar_df = trend_df.copy()
                    bar_df['date'] = bar_df['date'].dt.to_period(freq_map[agg_type]).dt.to_timestamp()
                    bar_df = bar_df.groupby(['date', 'topic']).size().reset_index(name='mentions')
                    
                    # ç»˜åˆ¶å †å æŸ±çŠ¶å›¾
                    fig_bar = px.bar(bar_df, x='date', y='mentions', color='topic',
                                    barmode='stack',
                                    template="plotly_white",
                                    color_discrete_sequence=JP_COLORS,
                                    labels={'date': 'æ—¥æœŸ', 'mentions': 'æåŠæ¬¡æ•°', 'topic': 'å…³æ³¨ç»´åº¦'})
                    fig_bar.update_layout(
                        title="æ ¸å¿ƒè¯é¢˜çƒ­åº¦æ¼”è¿› (åŒè½´å…³è”äº‹ä»¶)",
                        height=400,
                        font_family="Inter", title_font_family="Noto Serif JP",
                        margin=dict(t=10, b=0, l=0, r=0),
                        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
                        xaxis=dict(title=None),
                        yaxis=dict(title="æåŠæ¬¡æ•°")
                    )
                    
                    # Add Events to Topic Evolution
                    fig_bar = add_events_to_fig(fig_bar, events, start_date, end_date)
                    
                    st.plotly_chart(fig_bar, use_container_width=True)
                else:
                    st.info("ğŸ’¡ é€‰å®šæ—¶é—´èŒƒå›´å†…æš‚æ— è¯é¢˜æå–æ•°æ®ã€‚")
            else:
                st.info("ğŸ’¡ æš‚æ— è¯é¢˜æå–æ•°æ®ã€‚è¯·ç¡®ä¿å·²åœ¨çˆ¬è™«ç®¡ç†ä¸­æ‰§è¡Œâ€˜æ·±åº¦åˆ†æâ€™ã€‚")

        # 1.6 å…³é”®è¯é£™å‡æ¦œ (Anomaly Detection)
        st.markdown("---")
        st.subheader("ğŸš€ çƒ­è¯é£™å‡æ¦œ (Anomaly Detection)")
        st.caption("å¯¹æ¯”è¿‡å» 7 å¤©ä¸æ›´æ—© 21 å¤©ï¼ŒæŒ–æ˜è®¨è®ºçƒ­åº¦å¢é•¿æœ€å¿«çš„å…³é”®è¯")

        if not df.empty and 'content' in df.columns:
            from collections import Counter
            
            # 1. Split data into Recent vs Baseline
            # Use the latest date in the filtered dataframe as the reference point
            latest_date = df['review_date'].max().normalize()
            recent_cutoff = latest_date - pd.Timedelta(days=7)
            baseline_cutoff = latest_date - pd.Timedelta(days=28)
            
            df_recent = df[df['review_date'] >= recent_cutoff]
            df_baseline = df[(df['review_date'] < recent_cutoff) & (df['review_date'] >= baseline_cutoff)]
            
            if len(df_recent) > 5 and len(df_baseline) > 5:
                stopwords = load_stopwords()
                
                def get_word_freq(dataframe):
                    all_words = []
                    for _, row in dataframe.iterrows():
                        txt = row['content']
                        src = row.get('source', '').lower()
                        # Use smart_tokenize with source context
                        words = smart_tokenize(txt, source=src, stopwords=stopwords)
                        all_words.extend(words)
                    counts = Counter(all_words)
                    total = sum(counts.values())
                    return counts, total

                counts_r, total_r = get_word_freq(df_recent)
                counts_b, total_b = get_word_freq(df_baseline)
                
                # 2. Calculate Growth Score
                # Score = (Recent_Freq + epsilon) / (Baseline_Freq + epsilon)
                rising_data = []
                for word, c_r in counts_r.items():
                    if c_r < 3: continue # Filter noise: word must appear at least 3 times recently
                    
                    freq_r = c_r / total_r
                    c_b = counts_b.get(word, 0)
                    freq_b = c_b / total_b if total_b > 0 else 0
                    
                    # Growth logic: Simple multiple of frequency
                    # Use a small floor for baseline freq to avoid division by zero and over-weighting brand new words
                    floor_freq = 0.5 / total_b if total_b > 0 else 0.0001
                    growth = freq_r / max(freq_b, floor_freq)
                    
                    rising_data.append({
                        "å…³é”®è¯": word, 
                        "æœ€è¿‘æåŠ": c_r, 
                        "åŸºå‡†æåŠ": c_b, 
                        "å¢é•¿å€ç‡": round(growth, 1)
                    })
                
                if rising_data:
                    rising_df = pd.DataFrame(rising_data).sort_values("å¢é•¿å€ç‡", ascending=False).head(10)
                    
                    c1, c2 = st.columns([2, 1])
                    with c1:
                        fig_rising = px.bar(rising_df, x="å¢é•¿å€ç‡", y="å…³é”®è¯", orientation='h', 
                                           color="å¢é•¿å€ç‡", color_continuous_scale="Reds",
                                           title="Top 10 é£™å‡å…³é”®è¯",
                                           text="æœ€è¿‘æåŠ")
                        fig_rising.update_layout(yaxis={'categoryorder':'total ascending'}, showlegend=False)
                        st.plotly_chart(fig_rising, use_container_width=True)
                    with c2:
                        st.write("ğŸ“Š è¯¦æƒ…æ•°æ®")
                        st.dataframe(rising_df[["å…³é”®è¯", "å¢é•¿å€ç‡", "æœ€è¿‘æåŠ"]], hide_index=True)
                else:
                    st.info("å°šæœªå‘ç°æ˜æ˜¾çš„è¯é¢‘å¼‚åŠ¨ã€‚")
            else:
                st.info("æ•°æ®é‡ä¸è¶³ï¼ˆéœ€è¦è‡³å°‘ 7 å¤©ä»¥ä¸Šçš„å†å²æ•°æ®è¿›è¡Œå¯¹æ¯”ï¼‰ã€‚")



        # 3. Google Trends / Market Heat
        st.markdown("---")
        st.subheader("ğŸŒ å¸‚åœºçƒ­åº¦è¶‹åŠ¿ (Google Trends)")
        
        TRENDS_DB = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data', 'market_trends.db')
        if os.path.exists(TRENDS_DB):
            try:
                t_conn = sqlite3.connect(TRENDS_DB)
                t_df = pd.read_sql_query("SELECT * FROM google_trends", t_conn)
                t_conn.close()
                
                if not t_df.empty:
                    t_df['date'] = pd.to_datetime(t_df['date'])
                    
                    # Apply Global Date Filter
                    t_df = t_df[(t_df['date'].dt.date >= start_date) & (t_df['date'].dt.date <= end_date)]
                    
                    # Region Mapping for better display
                    region_map = {
                        'TW': 'å°æ¹¾', 'HK': 'é¦™æ¸¯', 
                        'US': 'ç¾å›½', 
                        'TH': 'æ³°å›½', 'JP': 'æ—¥æœ¬'
                    }

                    t_df['region_name'] = t_df['region'].map(region_map)
                    
                    # Filtering options for Trends
                    c1, c2 = st.columns([1, 3])
                    with c1:
                        selected_kw = st.selectbox("é€‰æ‹©çƒ­åº¦è¯", t_df['keyword'].unique())
                    
                    plot_df = t_df[t_df['keyword'] == selected_kw]
                    
                    fig_trend = px.line(plot_df, x='date', y='interest_score', color='region_name',
                                       title=f"'{selected_kw}' è¿‘æœŸæœç´¢çƒ­åº¦ (100ä¸ºå³°å€¼)",
                                       color_discrete_sequence=JP_COLORS,
                                       labels={'interest_score': 'æœç´¢çƒ­åº¦', 'date': 'æ—¥æœŸ', 'region_name': 'åœ°åŒº'})
                    fig_trend.update_layout(
                        font_family="Inter", 
                        title_font_family="Noto Serif JP",
                        legend=dict(orientation="v", yanchor="top", y=1, xanchor="left", x=1.02)
                    )
                    st.plotly_chart(fig_trend, use_container_width=True)
                else:
                    st.info("ğŸ“Š å¸‚åœºè¶‹åŠ¿æ•°æ®åº“æš‚æ— æ•°æ®ï¼Œè¯·è¿è¡Œ google_trends.py æŠ“å–ã€‚")
            except Exception as trend_e:
                st.error(f"è¶‹åŠ¿æ•°æ®åŠ è½½å¤±è´¥: {trend_e}")
        else:
            st.info("ğŸ’¡ å°šæœªåˆ›å»ºå¸‚åœºè¶‹åŠ¿æ•°æ®åº“ã€‚")

elif menu == "ğŸ§­ è¯­ä¹‰é€è§†ä¸æœç´¢":
    render_hero("Semantic Panorama", "è¯­ä¹‰é€è§†ä¸è¯„è®ºæœç´¢")
    
    tab1, tab2 = st.tabs(["ğŸ—ºï¸ è¯­ä¹‰åˆ†å¸ƒå›¾", "ğŸ” å…¨è¯­å¢ƒæœç´¢"])
    
    with tab1:
        st.markdown("""
        ğŸ’¡ **å¦‚ä½•é˜…è¯»æ­¤å›¾ï¼Ÿ**
        - æ¯ä¸€ä¸ªç‚¹ä»£è¡¨ä¸€æ¡ç©å®¶è¯„è®ºã€‚ä½ç½®è¶Šæ¥è¿‘çš„ç‚¹ï¼Œè¯­ä¹‰è¶Šç›¸ä¼¼ã€‚
        - ä¸åŒçš„é¢œè‰²ä»£è¡¨å¤§æ¨¡å‹è‡ªåŠ¨è¯†åˆ«å‡ºçš„ **â€œéšè—è¯é¢˜â€**ã€‚
        - **æ‚¬åœ** é¼ æ ‡å¯é¢„è§ˆå…·ä½“è¯„è®ºå†…å®¹ã€‚
        """)
        
        if df.empty or 'x' not in df.columns or df['x'].isnull().all():
            st.warning("ğŸ“Š è¯­ä¹‰åœ°å›¾æš‚æœªç”Ÿæˆã€‚")
            st.info("è¯·ç¡®ä¿å·²åœ¨ `.env` é…ç½® `GEMINI_API_KEY` å¹¶è¿è¡Œ `python scripts/process_semantic.py` ä»¥è¿›è¡Œå‘é‡åŒ–å¤„ç†ã€‚")
        else:
            # Filter for data that has coordinates
            map_df = df.dropna(subset=['x', 'y', 'cluster_label'])
            
            if map_df.empty:
                st.info("å°šæœªè¿›è¡Œå‘é‡åŒ–åˆ†æï¼Œè¯·è¿è¡Œåå°è„šæœ¬ã€‚")
            else:
                # 0. Pre-clean labels for display
                def clean_label_display(l):
                    if not isinstance(l, str): return l
                    l = re.sub(r'\(.*?\)', '', l) # Remove (Pinyin)
                    l = re.sub(r'ï¼ˆ.*?ï¼‰', '', l) # Remove (æ‹¬å·å†…å®¹)
                    return l.replace('****', '').strip()
                
                map_df['cluster_label'] = map_df['cluster_label'].apply(clean_label_display)

                # 1. Map rendering (Full Width)
                fig_map = px.scatter(
                    map_df, x='x', y='y',
                    color='cluster_label',
                    hover_data={'content': True, 'x': False, 'y': False, 'cluster_label': False},
                    title="å…¨é‡èˆ†æƒ…è¯­ä¹‰åˆ†å¸ƒå›¾",
                    template="plotly_white",
                    color_discrete_sequence=px.colors.qualitative.Prism,
                    height=650
                )
                
                fig_map.update_traces(marker=dict(size=8, opacity=0.7, line=dict(width=1, color='White')))
                fig_map.update_layout(
                    font_family="Inter", 
                    title_font_family="Noto Serif JP",
                    showlegend=True,
                    legend=dict(orientation="v", yanchor="top", y=1, xanchor="left", x=1.02, title="è¯é¢˜åˆ†ç±»"),
                    xaxis=dict(showgrid=False, showticklabels=False, title=""),
                    yaxis=dict(showgrid=False, showticklabels=False, title=""),
                    margin=dict(l=0, r=0, t=40, b=0)
                )
                st.plotly_chart(fig_map, use_container_width=True)
                
                # 2. Stats and search rendering (Below the map)
                st.markdown("---")
                st.subheader("ğŸ“‹ è¯é¢˜åˆ†å¸ƒè¯¦æƒ…")
                cluster_stats = map_df.groupby('cluster_label').size().reset_index(name='count')
                cluster_stats = cluster_stats.sort_values('count', ascending=False)
                
                st.dataframe(
                    cluster_stats,
                    column_config={
                        "cluster_label": "æ ¸å¿ƒè¯é¢˜å",
                        "count": st.column_config.NumberColumn("è¦†ç›–è¯„è®ºæ•°", format="%d ğŸ’¬")
                    },
                    hide_index=True,
                    use_container_width=True
                )

    with tab2:
        if not df.empty:
            search_col, sort_col = st.columns([3, 1])
            with search_col:
                search = st.text_input("è¾“å…¥å…³é”®è¯æœç´¢ï¼ˆæ”¯æŒæ­£åˆ™ï¼‰", placeholder="ä¾‹å¦‚: å»¶è¿Ÿ|å¡é¡¿")
                st.caption("ğŸ’¡ **æœç´¢æç¤º**ï¼šç³»ç»Ÿæš‚ä¸æ”¯æŒç®€ç¹ä½“è‡ªåŠ¨è½¬æ¢ã€‚è‹¥éœ€æœç´¢æ¸¯æ¾³å°/æµ·å¤–è¯„è®ºï¼Œè¯·ç¡®ä¿å…³é”®è¯ä¸åŸæ–‡æ–‡å­—æ ¼å¼ï¼ˆç®€/ç¹ï¼‰ä¸€è‡´ã€‚")
            with sort_col:
                sort_order = st.selectbox("æ’åºæ–¹å¼", ["æ—¶é—´å€’åº", "æ—¶é—´æ­£åº", "è¯„åˆ†ä»ä½åˆ°é«˜", "è¯„åˆ†ä»é«˜åˆ°ä½"])
            
            filtered_df = df
            if search:
                filtered_df = df[df['content'].str.contains(search, na=False, case=False, regex=True)]
            
            # Sort logic
            if sort_order == "æ—¶é—´å€’åº":
                filtered_df = filtered_df.sort_values('review_date', ascending=False)
            elif sort_order == "æ—¶é—´æ­£åº":
                filtered_df = filtered_df.sort_values('review_date', ascending=True)
            elif sort_order == "è¯„åˆ†ä»ä½åˆ°é«˜":
                filtered_df = filtered_df.sort_values('rating', ascending=True)
            elif sort_order == "è¯„åˆ†ä»é«˜åˆ°ä½":
                filtered_df = filtered_df.sort_values('rating', ascending=False)

            st.write(f"ğŸ” æ‰¾åˆ° {len(filtered_df)} æ¡åŒ¹é…è¯„è®º")
            st.markdown("---")
            
            # Pagination for search results to avoid lag
            items_per_page = 20
            num_pages = (len(filtered_df) // items_per_page) + 1 if len(filtered_df) > 0 else 1
            if num_pages > 1:
                page = st.number_input("é¡µç ", min_value=1, max_value=num_pages, step=1)
            else:
                page = 1
            
            start_idx = (page - 1) * items_per_page
            end_idx = start_idx + items_per_page
            
            for idx, row in filtered_df.iloc[start_idx:end_idx].iterrows():
                source_badge = f"**[{row.get('source', 'Unknown')}]**"
                date_display = str(row.get('review_date', '')).split(' ')[0]
                rating_val = row.get('rating', 0)
                rating_stars = "â­" * int(rating_val) if pd.notna(rating_val) and rating_val > 0 else "æ— è¯„åˆ†"
                
                st.markdown(f"{source_badge} **{row.get('author','Unknown')}** {rating_stars} | {date_display}")
                
                # Content Info
                c_title = row.get('content_title', '')
                c_url = row.get('content_url', '')
                
                if c_title:
                    st.markdown(f"ğŸ“º *æ¥æº*: [{c_title}]({c_url})")
                    
                st.info(row.get('content'))
                st.markdown("---")

elif menu == "ğŸ“š æ¼«ç”» IP å¤§ç›˜":
    render_hero("Manga IP Analysis", "IP åŠ¿åŠ›ä¸è§’è‰²çƒ­åº¦åˆ†æ")
    
    # Load Hero/IP Mapping
    hero_ip_map, ip_hero_list, hero_display_map = load_hero_ip_map(selected_game_key)
    
    # Process Data
    ip_df, hero_df = process_trends(df, hero_ip_map)
    
    if ip_df.empty:
        st.info("æš‚æ—  IP ç›¸å…³åˆ†ææ•°æ®ã€‚è¯·å…ˆæ‰§è¡Œâ€˜æ·±åº¦åˆ†æâ€™ä»¥æå–è§’è‰²æåŠã€‚")
    else:
        # Convert Hero Codes to Display Names
        hero_df['hero_name'] = hero_df['hero'].map(lambda x: hero_display_map.get(x, x))
        
        # ----------------------
        # 1. Manga IP Overview (Heat & Volume)
        # ----------------------
        st.subheader("ğŸ”¥ IP åŠ¿åŠ›æ€»è§ˆ")
        
        # Aggregate by IP
        ip_stats = ip_df.groupby('ip').agg({
            'count': 'sum',
            'sentiment': 'mean'
        }).reset_index()
        
        # Filter out "Unknown" if necessary
        ip_stats = ip_stats[ip_stats['ip'] != 'Unknown']
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
             # Scatte Plot: Sentiment vs Volume
             fig_ip = px.scatter(ip_stats, x='sentiment', y='count',
                                size='count', color='ip',
                                text='ip',
                                labels={'sentiment': 'å¹³å‡æƒ…æ„Ÿ (0-1)', 'count': 'è¯„è®ºæåŠé‡'},
                                title='IP å£°é‡ä¸æƒ…æ„Ÿåˆ†å¸ƒ',
                                color_discrete_sequence=JP_COLORS,
                                template="plotly_white")
             fig_ip.update_traces(textposition='top center')
             fig_ip.update_layout(
                 legend=dict(orientation="v", yanchor="top", y=1, xanchor="left", x=1.02)
             )
             st.plotly_chart(fig_ip, use_container_width=True)
             
        with col2:
             # Ranking Table
             st.write("ğŸ† çƒ­é—¨ IP æ’è¡Œ")
             st.dataframe(
                 ip_stats.sort_values('count', ascending=False)[['ip', 'count', 'sentiment']],
                 column_config={
                     "sentiment": st.column_config.ProgressColumn("æƒ…æ„Ÿåˆ†", min_value=0, max_value=1, format="%.2f"),
                     "count": st.column_config.NumberColumn("çƒ­åº¦")
                 },
                 hide_index=True,
                 use_container_width=True
             )
             
        st.markdown("---")
        
        # ----------------------
        # 2. Character Stacked Sentiment
        # ----------------------
        st.subheader("ğŸ­ è§’è‰²æƒ…æ„Ÿæ„æˆ (å †å å›¾)")
        
        # Filter Selector (Optional)
        f_col1, _ = st.columns([1, 1])
        all_ips = sorted(ip_stats['ip'].unique())
        with f_col1:
            selected_ips = st.multiselect("IP ç­›é€‰ (ä¸é€‰åˆ™é»˜è®¤å…¨é€‰)", all_ips, default=[])
        
        # Apply Filter
        plot_df = hero_df.copy()
        if selected_ips:
            plot_df = plot_df[plot_df['ip'].isin(selected_ips)]
        
        if not plot_df.empty:
            # Calculate Sentiment Buckets
            NEG_TH = 0.4
            POS_TH = 0.6
            
            hero_agg = plot_df.groupby('hero_name')['sentiment'].apply(list).reset_index()
            
            stacked_data = []
            for _, row in hero_agg.iterrows():
                scores = row['sentiment']
                pos = sum(1 for s in scores if s > POS_TH)
                neu = sum(1 for s in scores if NEG_TH <= s <= POS_TH)
                neg = sum(1 for s in scores if s < NEG_TH)
                total = len(scores)
                
                stacked_data.append({
                    "hero": row['hero_name'],
                    "Positive": pos,
                    "Neutral": neu,
                    "Negative": neg,
                    "Total": total
                })
            
            # Sort by Total Volume Descending
            stack_df = pd.DataFrame(stacked_data).sort_values('Total', ascending=False)
            
             # Limit to Top 50 if too many
            if len(stack_df) > 50:
                st.caption(f"æ•°æ®æ˜¾ç¤º Top 50 çƒ­é—¨è§’è‰² (å…± {len(stack_df)} ä¸ª)")
                stack_df = stack_df.head(50)
            
            fig_stack = go.Figure()
            
            # Vertical Bars: X = Hero, Y = Count
            fig_stack.add_trace(go.Bar(
                x=stack_df['hero'], y=stack_df['Negative'], name='è´Ÿé¢', 
                marker_color='#e74c3c'
            ))
            fig_stack.add_trace(go.Bar(
                x=stack_df['hero'], y=stack_df['Neutral'], name='ä¸­æ€§', 
                marker_color='#95a5a6'
            ))
            fig_stack.add_trace(go.Bar(
                x=stack_df['hero'], y=stack_df['Positive'], name='æ­£é¢', 
                marker_color='#2ecc71'
            ))
            
            fig_stack.update_layout(
                barmode='stack',
                title='è§’è‰²çƒ­åº¦ä¸æƒ…æ„Ÿæ„æˆ',
                xaxis_title='è§’è‰²',
                yaxis_title='æåŠæ¬¡æ•°',
                template='plotly_white',
                height=500,
                xaxis_tickangle=-45,
                legend=dict(orientation="v", yanchor="top", y=1, xanchor="left", x=1.02)
            )
            st.plotly_chart(fig_stack, use_container_width=True)
            
            with st.expander("æŸ¥çœ‹è¯¦ç»†æ•°æ®è¡¨"):
                st.dataframe(
                    stack_df,
                    column_config={
                        "hero": "è§’è‰²åç§°",
                        "Total": st.column_config.NumberColumn("æåŠæ€»æ•°"),
                        "Positive": st.column_config.NumberColumn("æ­£é¢åé¦ˆ"),
                        "Neutral": st.column_config.NumberColumn("ä¸­æ€§åé¦ˆ"),
                        "Negative": st.column_config.NumberColumn("è´Ÿé¢åé¦ˆ")
                    },
                    use_container_width=True
                )
            
        else:
            st.info(f"å½“å‰ç­›é€‰æ¡ä»¶ä¸‹æš‚æ— è§’è‰²æ•°æ®")

elif menu == "ğŸ¦¸ è‹±é›„ä¸“é¡¹":

    render_hero("Hero Feedback", "è‹±é›„ä¸“é¡¹åé¦ˆ")
    
    if df.empty or 'detailed_analysis' not in df.columns:
        st.info("æš‚æ— è¯¦ç»†åˆ†ææ•°æ®")
    else:
        # Aggregate Hero Data
        hero_data = {} # {HeroName: {Dim: [items]}}
        
        for json_str in df['detailed_analysis'].dropna():
            try:
                data = json.loads(json_str)
                heroes = data.get("Heroes", {})
                for h_name, dims in heroes.items():
                    if h_name not in hero_data: hero_data[h_name] = {}
                    for dim, items in dims.items():
                        if dim not in hero_data[h_name]: hero_data[h_name][dim] = []
                        hero_data[h_name][dim].extend(items)
            except:
                pass
        
        if not hero_data:
            st.warning("æš‚æ— ç‰¹å®šè‹±é›„çš„åé¦ˆæ•°æ®ã€‚")
        else:
            # Load Groups from heroes.json for filtering
            config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config', 'heroes.json')
            hero_groups = {}
            if os.path.exists(config_path):
                try:
                    with open(config_path, 'r', encoding='utf-8') as f:
                        fc = json.load(f)
                        if selected_game_key in fc and "Groups" in fc[selected_game_key]:
                             raw_groups = fc[selected_game_key]["Groups"]
                             # Convert new Dict structure to List for UI filtering
                             # {GroupName: {Hero: [Aliases]}} -> {GroupName: [Hero]}
                             for g_name, g_heroes in raw_groups.items():
                                 if isinstance(g_heroes, dict):
                                     hero_groups[g_name] = list(g_heroes.keys())
                                 elif isinstance(g_heroes, list):
                                     hero_groups[g_name] = g_heroes
                                     
                             pass
                except: pass

            # Create a display map: CodeName -> Display Name
            display_map = {}
            hero_to_first_alias = {}
            
            # Load first aliases from heroes.json for best display names
            if os.path.exists(config_path):
                try:
                    with open(config_path, 'r', encoding='utf-8') as f:
                        fc = json.load(f)
                        if selected_game_key in fc and "Groups" in fc[selected_game_key]:
                             groups = fc[selected_game_key]["Groups"]
                             for g_heroes in groups.values():
                                 if isinstance(g_heroes, dict):
                                     for h_code, aliases in g_heroes.items():
                                         if aliases:
                                             hero_to_first_alias[h_code] = aliases[0]
                except: pass

            for h_code in hero_data.keys():
                # Priority 1: First alias from heroes.json (usually Simplified Chinese Full Name)
                if h_code in hero_to_first_alias:
                    display_map[h_code] = hero_to_first_alias[h_code]
                else:
                    # Fallback login: find shortest Chinese alias from keywords
                    keywords = GAMES[selected_game_key].get('keywords', {})
                    aliases = [k for k, v in keywords.items() if v == h_code]
                    chinese_aliases = [a for a in aliases if not re.search('[a-zA-Z]', a)]
                    if chinese_aliases:
                        display_map[h_code] = sorted(chinese_aliases, key=len)[0]
                    elif aliases:
                        display_map[h_code] = aliases[0]
                    else:
                        display_map[h_code] = h_code
            
            # Group Selection
            # 2026-01-07: Filter out heroes that are NOT in a configured group (remove unwanted IPs from UI)
            all_configured_heroes = set()
            if hero_groups:
                for h_list in hero_groups.values():
                    all_configured_heroes.update(h_list)
            
            selected_group_heroes = [h for h in hero_data.keys() if h in all_configured_heroes]
            
            # stragglers/others logic (if any are left that we actually want)
            ungrouped_heroes = [] # Force empty as we don't want to show unconfigured IPs

            if hero_groups:
                # Groups are sorted keys now
                group_names = ["å…¨éƒ¨"] + sorted(list(hero_groups.keys()))
                
                # Add "Others" option if there are stragglers
                if ungrouped_heroes:
                    group_names.append("å…¶ä»– - æœŸå¾…è”åŠ¨")
                
                # Side-by-side selector layout
                f_c1, f_c2 = st.columns(2)
                
                with f_c1:
                    selected_group = st.selectbox("é€‰æ‹©IPç³»åˆ— (Anime Source)", group_names)
                
                # Filter Logic
                if selected_group == "å…¶ä»– - æœŸå¾…è”åŠ¨":
                    selected_group_heroes = ungrouped_heroes
                elif selected_group != "å…¨éƒ¨":
                    # Filter heroes belonging to this group
                    allowed_heroes = set(hero_groups[selected_group])
                    selected_group_heroes = [h for h in hero_data.keys() if h in allowed_heroes]
                else:
                    # 'å…¨éƒ¨': re-select all configured
                    selected_group_heroes = [h for h in hero_data.keys() if h in all_configured_heroes]
            
            if not selected_group_heroes:
                st.warning("è¯¥ç³»åˆ—æš‚æ— ç›¸å…³åé¦ˆæ•°æ®çš„è‹±é›„ã€‚")
                selected_hero_code = None
            else:
                # Sort by Display Name
                sorted_heroes = sorted(selected_group_heroes, key=lambda x: display_map.get(x, x))
                
                with f_c2:
                    selected_hero_code = st.selectbox("é€‰æ‹©è§’è‰²", sorted_heroes, format_func=lambda x: display_map.get(x, x))
                
                if selected_hero_code:
                    st.subheader(f"âš”ï¸ {display_map.get(selected_hero_code, selected_hero_code)}")
                    
                    # --- Hero Trend Chart ---
                    # 1. Load Data for Trends
                    hero_ip_map, _, _ = load_hero_ip_map(selected_game_key)
                    _, hero_trend_df = process_trends(df, hero_ip_map)
                    
                    if not hero_trend_df.empty:
                        # Aggregation Selector
                        agg_type_h = st.radio("è¶‹åŠ¿æ—¶é—´ç²’åº¦", ["å‘¨ (Weekly)", "æ—¥ (Daily)", "æœˆ (Monthly)"], key="hero_trend_agg", horizontal=True)
                        freq_map_h = {"å‘¨ (Weekly)": "W-MON", "æ—¥ (Daily)": "D", "æœˆ (Monthly)": "M"}
                        freq_code_h = freq_map_h[agg_type_h]
                        
                        this_hero_df = hero_trend_df[hero_trend_df['hero'] == selected_hero_code]
                        
                        if not this_hero_df.empty:
                            # Group by Date-Freq
                            h_stats = this_hero_df.groupby([pd.Grouper(key='date', freq=freq_code_h)]).agg({
                                'count': 'count',
                                'sentiment': 'mean'
                            }).reset_index()
                            
                            # Treat 0 volume as missing for dashed connection
                            h_stats['count'] = h_stats['count'].replace(0, None)
                            
                            fig_h = go.Figure()
                            # Volume
                            fig_h.add_trace(go.Scatter(
                                x=h_stats['date'], y=h_stats['count'], name='çƒ­åº¦ (Mentions)',
                                line=dict(color=JP_COLORS[0], width=2, dash='dot'),
                                connectgaps=True,
                                mode='lines+markers'
                            ))
                            # Sentiment
                            fig_h.add_trace(go.Scatter(
                                x=h_stats['date'], y=h_stats['sentiment'], name='å¹³å‡æƒ…æ„Ÿ',
                                line=dict(color=JP_COLORS[1], width=3, dash='dot'),
                                connectgaps=True,
                                yaxis='y2',
                                mode='lines+markers'
                            ))
                            
                            layout_args = dict(
                                title=f"{display_map.get(selected_hero_code, selected_hero_code)} - çƒ­åº¦ä¸æƒ…æ„Ÿèµ°åŠ¿",
                                xaxis=dict(
                                    title=None,
                                    tickformat="%Y-%m-%d", # Force date format to avoid 00:00:00.001
                                ),
                                yaxis=dict(title='çƒ­åº¦', showgrid=False),
                                yaxis2=dict(title='æƒ…æ„Ÿ', overlaying='y', side='right', range=[0, 1], showgrid=True),
                                legend=dict(orientation="v", yanchor="top", y=1, xanchor="left", x=1.02),
                                height=350,
                                template='plotly_white'
                            )

                            # Optimize display for single data point
                            if len(h_stats) == 1:
                                one_date = h_stats['date'].iloc[0]
                                layout_args['xaxis']['range'] = [
                                    one_date - pd.Timedelta(days=1),
                                    one_date + pd.Timedelta(days=1)
                                ]

                            fig_h.update_layout(**layout_args)
                            st.plotly_chart(fig_h, use_container_width=True)

                    dims = hero_data[selected_hero_code]
                    
                    tabs = st.tabs(["ğŸ’­ ç»¼åˆ", "ğŸ—¡ï¸ æŠ€èƒ½", "ğŸ¨ å½¢è±¡", "ğŸ’ª å¼ºåº¦", "ğŸ”— å…³è”è¯ç½‘"])
                    
                    def render_feedback(dimension_key, tab_container):
                        with tab_container:
                            items = dims.get(dimension_key, [])
                            if not items:
                                st.caption("æš‚æ— ç›¸å…³åé¦ˆ")
                                return
                            
                            # Deduplicate display
                            seen = set()
                            unique_items = []
                            for item in items:
                                txt_norm = item['text'].strip()
                                if txt_norm not in seen:
                                    seen.add(txt_norm)
                                    unique_items.append(item)
                            
                            pos = [i for i in unique_items if i['label'] == 'Positive']
                            neg = [i for i in unique_items if i['label'] == 'Negative']
                            
                            c1, c2 = st.columns(2)
                            with c1:
                                st.write(f"ğŸ™‚ å¥½è¯„ ({len(pos)})")
                                for p in pos:
                                    meta = p.get('metadata')
                                    tooltip = format_tooltip(meta)
                                    st.markdown(f'<div class="feedback-box feedback-pos" title="{tooltip}">{html.escape(p["text"])}</div>', unsafe_allow_html=True)
                            with c2:
                                st.write(f"ğŸ˜¡ å·®è¯„/å»ºè®® ({len(neg)})")
                                for n in neg:
                                    meta = n.get('metadata')
                                    tooltip = format_tooltip(meta)
                                    st.markdown(f'<div class="feedback-box feedback-neg" title="{tooltip}">{html.escape(n["text"])}</div>', unsafe_allow_html=True)

                    
                    render_feedback("General", tabs[0])
                    render_feedback("Skill", tabs[1])
                    render_feedback("Visual", tabs[2])
                    render_feedback("Strength", tabs[3])

                    # --- New Tab: Keyword Co-occurrence Network ---
                    with tabs[4]:
                        st.subheader("ğŸ”— æ ¸å¿ƒå…³è”è¯ç½‘ç»œ")
                        st.caption("åˆ†æç©å®¶åœ¨è®¨è®ºè¯¥è‹±é›„æ—¶ï¼Œæœ€å¸¸è”æƒ³åˆ°çš„è¯æ±‡ç»„åˆã€‚")
                        
                        # Collect all related texts
                        hero_texts = []
                        for dim_key in dims:
                            for item in dims[dim_key]:
                                hero_texts.append(item['text'])
                        
                        if len(hero_texts) < 3:
                            st.info("æ•°æ®é‡è¾ƒå°‘ï¼Œæ— æ³•ç”Ÿæˆå…³è”ç½‘ç»œã€‚")
                        else:
                            stopwords = load_stopwords()
                            # 1. Tokenize and clean
                            tokenized_docs = []
                            for idx, row in enumerate(hero_texts):
                                # Since hero_texts is a list, we might not have 'source' easily here
                                # But we can pass the source if we change how hero_texts is collected
                                words = smart_tokenize(row, stopwords=stopwords)
                                if words: tokenized_docs.append(list(set(words))) # Unique words per sentence
                            
                            # 2. Count Co-occurrences
                            pair_counts = Counter()
                            word_freq = Counter()
                            for doc in tokenized_docs:
                                for word in doc: word_freq[word] += 1
                                if len(doc) >= 2:
                                    for pair in itertools.combinations(sorted(doc), 2):
                                        pair_counts[pair] += 1
                            
                            if not pair_counts:
                                st.info("æœªå‘ç°æ˜¾è‘—çš„è¯æ±‡å…³è”ã€‚")
                            else:
                                # 3. Prepare Graph Data (Top 30 edges)
                                top_pairs = pair_counts.most_common(30)
                                nodes = set()
                                for p, c in top_pairs:
                                    nodes.add(p[0])
                                    nodes.add(p[1])
                                
                                # 4. Simple Circular Layout (Manual calculation for performance)
                                import math
                                node_list = list(nodes)
                                pos = {node: (math.cos(2*math.pi*i/len(node_list)), math.sin(2*math.pi*i/len(node_list))) for i, node in enumerate(node_list)}
                                
                                # 5. Draw Edges
                                edge_x, edge_y = [], []
                                for (u, v), w in top_pairs:
                                    edge_x.extend([pos[u][0], pos[v][0], None])
                                    edge_y.extend([pos[u][1], pos[v][1], None])
                                
                                edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(width=1, color='#ddd'), hoverinfo='none', mode='lines')
                                
                                # 6. Draw Nodes
                                node_x, node_y, node_text, node_size = [], [], [], []
                                for node in node_list:
                                    node_x.append(pos[node][0])
                                    node_y.append(pos[node][1])
                                    node_text.append(f"{node} (æåŠ:{word_freq[node]})")
                                    # Scale size based on frequency
                                    node_size.append(max(10, min(40, word_freq[node] * 3)))
                                
                                node_trace = go.Scatter(
                                    x=node_x, y=node_y, mode='markers+text',
                                    text=[n for n in node_list],
                                    textposition="top center",
                                    hoverinfo='text', hovertext=node_text,
                                    marker=dict(size=node_size, color='#9be9a8', line_width=2, line_color='#40c463')
                                )
                                
                                fig_net = go.Figure(data=[edge_trace, node_trace],
                                     layout=go.Layout(
                                        showlegend=False, hovermode='closest',
                                        margin=dict(b=0,l=0,r=0,t=40),
                                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                                        height=500,
                                        plot_bgcolor='rgba(0,0,0,0)'
                                    ))
                                st.plotly_chart(fig_net, use_container_width=True)

elif menu == "âš™ï¸ ç©æ³•åé¦ˆ":
    render_hero("Gameplay & System", "ç©æ³•ä¸ç³»ç»Ÿåé¦ˆ")
    
    if df.empty or 'detailed_analysis' not in df.columns:
        st.info("æš‚æ— æ•°æ®")
    else:
        sys_data = {}
        # Define keywords to filter out official posts
        official_filter_keywords = [
            "æ´»åŠ¨æœŸé—´", "æ´»åŠ¨æ—¶é—´", "æ´»åŠ¨æœ‰æ•ˆæœŸ", "è´´å§ä¸“å±ç¦åˆ©", "æˆªæ­¢è‡³", 
            "è·å¾—**å¥–åŠ±ç”¨æˆ·åå•", "ä¸¥ç¦è®¨è®º", "å§å‹å‚ä¸æ´»åŠ¨", 
            "åŠ å…¥ç©å®¶äº¤æµç¾¤", "å®è´µåé¦ˆå’Œå»ºè®®", "ç›–æ¥¼é€ç¦åˆ©", "å®˜æ–¹å§å§åŠ¡ç»„","è¡·å¿ƒæ„Ÿè°¢å¤§å®¶"
        ]

        for json_str in df['detailed_analysis'].dropna():
            try:
                data = json.loads(json_str)
                system = data.get("System", {})
                for aspect, items in system.items():
                    if aspect not in sys_data: sys_data[aspect] = []
                    
                    # Filter items
                    filtered_items = []
                    for item in items:
                        text_content = item.get('text', '')
                        # Check if any official keyword is in the text
                        if not any(kw in text_content for kw in official_filter_keywords):
                            filtered_items.append(item)
                            
                    sys_data[aspect].extend(filtered_items)
            except:
                pass
            
        sorted_keys = sorted(sys_data.keys())
        emoji_map = {
            "Matchmaking": "âš–ï¸ Matchmaking", 
            "Network": "ğŸ“¡ Network", 
            "Optimization": "ğŸš€ Optimization", 
            "Welfare": "ğŸ Welfare",
            "Gameplay": "ğŸ® Gameplay",
            "Visuals": "ğŸ¨ Visuals"
        }
        tab_labels = [emoji_map.get(k, k) for k in sorted_keys]

        tabs = st.tabs(tab_labels) if sys_data else []
        if not tabs:
            st.warning("æš‚æ— ç³»ç»Ÿå±‚é¢çš„åé¦ˆã€‚")
        else:
            for i, aspect in enumerate(sorted_keys):
                with tabs[i]:
                    items = sys_data[aspect]
                    
                    # Deduplicate display
                    seen = set()
                    unique_items = []
                    for item in items:
                        txt_norm = item['text'].strip()
                        if txt_norm not in seen:
                            seen.add(txt_norm)
                            unique_items.append(item)

                    pos = [x for x in unique_items if x['label'] == 'Positive']
                    neg = [x for x in unique_items if x['label'] == 'Negative']
                    c1, c2 = st.columns(2)
                    with c1:
                        st.subheader(f"æ­£é¢ ({len(pos)})")
                        # Show more items and sort by length to prioritize descriptive reviews
                        pos_sorted = sorted(pos, key=lambda x: len(x['text']), reverse=True)
                        for x in pos_sorted[:150]:
                             meta = x.get('metadata')
                             tooltip = format_tooltip(meta)
                             tag_html = "".join([f"<span class='mode-tag'>{tag}</span>" for tag in x.get('tags', [])])
                             st.markdown(f'<div class="feedback-box feedback-pos" title="{tooltip}">{tag_html}{html.escape(x["text"])}</div>', unsafe_allow_html=True)
                    with c2:
                        st.subheader(f"è´Ÿé¢/é—®é¢˜ ({len(neg)})")
                        neg_sorted = sorted(neg, key=lambda x: len(x['text']), reverse=True)
                        for x in neg_sorted[:150]:
                             meta = x.get('metadata')
                             tooltip = format_tooltip(meta)
                             tag_html = "".join([f"<span class='mode-tag'>{tag}</span>" for tag in x.get('tags', [])])
                             st.markdown(f'<div class="feedback-box feedback-neg" title="{tooltip}">{tag_html}{html.escape(x["text"])}</div>', unsafe_allow_html=True)



elif menu == "ğŸ“„ åˆ†ææœˆæŠ¥":
    render_hero("Monthly Analysis Report", "èˆ†æƒ…åˆ†ææœˆæŠ¥")
    st.info("å±•ç¤ºå·²ç”Ÿæˆçš„å‘¨æœŸæ€§åˆ†ææŠ¥å‘Šã€‚å¦‚éœ€ç”Ÿæˆæ–°æŠ¥å‘Šï¼Œè¯·åœ¨åå°è¿è¡Œ `python main.py report`ã€‚")
    
    # Define reports directory
    reports_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'reports')
    if not os.path.exists(reports_dir):
        os.makedirs(reports_dir)
        # Also check root for legacy reports
        root_dir = os.path.dirname(os.path.dirname(__file__))
        for f in os.listdir(root_dir):
            if f.startswith("public_opinion_report_") and f.endswith(".md"):
                import shutil
                shutil.move(os.path.join(root_dir, f), os.path.join(reports_dir, f))

    # List all markdown reports
    report_files = sorted([f for f in os.listdir(reports_dir) if f.endswith(".md")], reverse=True)
    
    if not report_files:
        st.warning("æš‚æ— å·²ç”Ÿæˆçš„æŠ¥å‘Šã€‚")
    else:
        # Create a nice display name for the selector
        # e.g., public_opinion_report_202511.md -> 2025å¹´11æœˆ æ·±åº¦æŠ¥å‘Š
        display_names = []
        for f in report_files:
            date_match = re.search(r'(\d{4})(\d{2})', f)
            if date_match:
                display_names.append(f"ğŸ“… {date_match.group(1)}å¹´{date_match.group(2)}æœˆ åˆ†ææŠ¥å‘Š")
            else:
                display_names.append(f"ğŸ“„ {f}")
        
        selected_display = st.selectbox("é€‰æ‹©æŠ¥å‘Šç‰ˆæœ¬", display_names)
        selected_file = report_files[display_names.index(selected_display)]
        
        st.markdown("---")
        
        # Read and display the report
        report_path = os.path.join(reports_dir, selected_file)
        with open(report_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # Display content
        st.markdown(content)
        
        # Download button
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½ Markdown æŠ¥å‘Š",
            data=content,
            file_name=selected_file,
            mime="text/markdown"
        )

elif menu == "ğŸ”§ é…ç½®ç®¡ç†":
    render_hero("System Configuration", "ç³»ç»Ÿé…ç½®ä¸­å¿ƒ")
    
    st.subheader("ğŸŒ çˆ¬è™«æºç›‘æ§åˆ—è¡¨")
    st.caption(f"å½“å‰é¡¹ç›®: {selected_game_name} ({selected_game_key})")
    st.info("æ­¤åˆ—è¡¨å±•ç¤ºå½“å‰ç”Ÿæ•ˆçš„çˆ¬è™«ç›®æ ‡é“¾æ¥ã€‚å¦‚éœ€æ–°å¢æˆ–ä¿®æ”¹ï¼Œè¯·ç¼–è¾‘ `config/settings.py` æ–‡ä»¶ã€‚")

    if selected_game_key in GAMES:
        urls = GAMES[selected_game_key].get("urls", [])
        if urls:
            for i, u in enumerate(urls):
                st.text_input(f"Source {i+1}", u, disabled=True, key=f"src_{i}")
        else:
            st.warning("æš‚æœªé…ç½®ä»»ä½•æŠ“å–é“¾æ¥ã€‚")
    else:
        st.error("æ— æ³•è¯»å–å½“å‰é¡¹ç›®é…ç½®ã€‚")
    
    st.markdown("---")

    st.subheader("ğŸ¦¸ è‹±é›„ä¸“é¡¹é…ç½®")
    st.info("åœ¨è¿™é‡Œç¼–è¾‘æ¸¸æˆã€è‹±é›„åŠå…¶åˆ«åã€‚")
    
    config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config', 'heroes.json')
    
    # Load current config
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            current_config = json.load(f)
    else:
        current_config = {}
        st.warning("Config file not found, creating new.")
    
    edited_json = st.text_area("Heroes Config (JSON)", json.dumps(current_config, indent=4, ensure_ascii=False), height=400)
    
    if st.button("ä¿å­˜é…ç½®"):
        try:
            new_config = json.loads(edited_json)
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(new_config, f, indent=4, ensure_ascii=False)
            st.success("é…ç½®å·²ä¿å­˜ï¼")
            
            # Update the global GAMES config or trigger a reload if necessary
            # For now, just confirming save. The analysis logic needs to reload this.
        except json.JSONDecodeError:
            st.error("JSON æ ¼å¼é”™è¯¯ï¼Œè¯·æ£€æŸ¥è¯­æ³•ã€‚")

    st.markdown("---")
    st.subheader("ğŸš« åœç”¨è¯ç®¡ç† (Stopwords)")
    st.info("åœ¨è¿™é‡Œç¼–è¾‘å…³é”®è¯åˆ†æä¸­éœ€è¦å¿½ç•¥çš„åœç”¨è¯ï¼Œæ¯è¡Œä¸€ä¸ªè¯ã€‚")
    
    stop_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config', 'stopwords.txt')
    current_stopwords = ""
    if os.path.exists(stop_path):
        with open(stop_path, 'r', encoding='utf-8') as f:
            current_stopwords = f.read()
    
    edited_stopwords = st.text_area("åœç”¨è¯åˆ—è¡¨", current_stopwords, height=300)
    
    if st.button("ä¿å­˜åœç”¨è¯"):
        try:
            with open(stop_path, 'w', encoding='utf-8') as f:
                f.write(edited_stopwords)
            st.success("åœç”¨è¯å·²ä¿å­˜ï¼")
        except Exception as e:
            st.error(f"ä¿å­˜å¤±è´¥: {e}")

    st.markdown("---")
    st.subheader("ğŸ“… é‡å¤§äº‹ä»¶ç®¡ç† (Events)")
    st.info("ç¼–è¾‘ç”¨äºè¶‹åŠ¿å›¾æ ‡æ³¨çš„é‡å¤§äº‹ä»¶ã€‚æ ¼å¼ä¸º JSON æ•°ç»„ï¼ŒåŒ…å« name, start, end å’Œ color (RGBA)ã€‚")

    events_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config', 'events.json')
    current_events_str = "[]"
    if os.path.exists(events_path):
        try:
            with open(events_path, 'r', encoding='utf-8') as f:
                events_data = json.load(f)
                current_events_str = json.dumps(events_data, indent=4, ensure_ascii=False)
        except: pass
    
    edited_events = st.text_area("äº‹ä»¶åˆ—è¡¨ (JSON)", current_events_str, height=300)
    
    if st.button("ä¿å­˜äº‹ä»¶"):
        try:
            # Validate JSON
            new_events = json.loads(edited_events)
            if not isinstance(new_events, list):
                st.error("æ ¼å¼é”™è¯¯ï¼šå¿…é¡»æ˜¯ä¸€ä¸ª JSON æ•°ç»„ã€‚")
            else:
                with open(events_path, 'w', encoding='utf-8') as f:
                    json.dump(new_events, f, indent=4, ensure_ascii=False)
                st.success("äº‹ä»¶é…ç½®å·²ä¿å­˜ï¼åˆ·æ–°æ€»è§ˆå¤§å±å³å¯æŸ¥çœ‹æ ‡æ³¨ã€‚")
        except json.JSONDecodeError:
            st.error("JSON æ ¼å¼é”™è¯¯ï¼Œè¯·æ£€æŸ¥è¯­æ³•ã€‚")
        except Exception as e:
            st.error(f"ä¿å­˜å¤±è´¥: {e}")
    
    # Simple Preview of Events
    if os.path.exists(events_path):
        try:
            with open(events_path, 'r', encoding='utf-8') as f:
                preview_evs = json.load(f)
                if preview_evs:
                    st.write("ğŸ” **å½“å‰äº‹ä»¶æ¦‚è§ˆ**:")
                    for ev in preview_evs:
                        st.write(f"- **{ev['name']}**: `{ev['start']}` åˆ° `{ev['end']}`")
        except: pass

